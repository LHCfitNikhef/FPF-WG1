{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4196a694-2798-4b26-b454-f391a8d7c9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284b53e8-dbad-45ef-973d-c92cb50a5e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(87654321)\n",
    "CURR_PATH = pathlib.Path().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1097bb54-2f73-493e-a2f8-132f858475a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_txt_pd(exp: str = \"FASERv2FCC_wide\", otype: str = \"inclusive\", charge: str = \"nu\") -> pd.DataFrame:\n",
    "    \n",
    "    path = f\"clipped_nan_binned_sysevents_{exp}_{otype}_{charge}.txt\"\n",
    "    fpath = CURR_PATH.joinpath(f\"stat_syst_uncertainties/{path}\")\n",
    "    \n",
    "    colnames = [\n",
    "        \"x_lower\", \n",
    "        \"x_upper\", \n",
    "        \"x_avg\", \n",
    "        \"Q2_lower\", \n",
    "        \"Q2_upper\", \n",
    "        \"Q2_avg\", \n",
    "        \"E_nu_lower\", \n",
    "        \"E_nu_upper\", \n",
    "        \"E_nu_avg\", \n",
    "        \"d^sigma/dxdQ2\", \n",
    "        \"N_events\", \n",
    "        \"N_events_errs\", \n",
    "        \"N_sys_errs\", \n",
    "        \"Percent_error_theta\", \n",
    "        \"Percent_error_Elepton\", \n",
    "        \"Percent_error_Ehadron\", \n",
    "        \"MC_Samples\",\n",
    "    ]\n",
    "    \n",
    "    return pd.read_csv(fpath, skiprows=2, delim_whitespace=True, names=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7ad77d5-6ad3-45a4-b687-9326f09ff028",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_lower</th>\n",
       "      <th>x_upper</th>\n",
       "      <th>x_avg</th>\n",
       "      <th>Q2_lower</th>\n",
       "      <th>Q2_upper</th>\n",
       "      <th>Q2_avg</th>\n",
       "      <th>E_nu_lower</th>\n",
       "      <th>E_nu_upper</th>\n",
       "      <th>E_nu_avg</th>\n",
       "      <th>d^sigma/dxdQ2</th>\n",
       "      <th>N_events</th>\n",
       "      <th>N_events_errs</th>\n",
       "      <th>N_sys_errs</th>\n",
       "      <th>Percent_error_theta</th>\n",
       "      <th>Percent_error_Elepton</th>\n",
       "      <th>Percent_error_Ehadron</th>\n",
       "      <th>MC_Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.390570</td>\n",
       "      <td>11.33635</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>15037.280</td>\n",
       "      <td>68.08121</td>\n",
       "      <td>1914.486</td>\n",
       "      <td>43.75484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.257304</td>\n",
       "      <td>11.33635</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>12771.790</td>\n",
       "      <td>39.49984</td>\n",
       "      <td>10880.890</td>\n",
       "      <td>104.31150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.457032</td>\n",
       "      <td>11.33635</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>8705.647</td>\n",
       "      <td>28.58960</td>\n",
       "      <td>21806.780</td>\n",
       "      <td>147.67120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14.695920</td>\n",
       "      <td>11.33635</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>14379.890</td>\n",
       "      <td>29.13603</td>\n",
       "      <td>6594.826</td>\n",
       "      <td>81.20854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.082763</td>\n",
       "      <td>11.33635</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>8678.035</td>\n",
       "      <td>21.33828</td>\n",
       "      <td>40198.550</td>\n",
       "      <td>200.49580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88220.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x_lower   x_upper     x_avg  Q2_lower  Q2_upper     Q2_avg  E_nu_lower  \\\n",
       "0  0.000189  0.000259  0.000230       4.0      10.0   5.390570    11.33635   \n",
       "1  0.000356  0.000489  0.000435       4.0      10.0   6.257304    11.33635   \n",
       "2  0.000489  0.000672  0.000557       4.0      10.0   5.457032    11.33635   \n",
       "3  0.000489  0.000672  0.000634      10.0     100.0  14.695920    11.33635   \n",
       "4  0.000672  0.000924  0.000804       4.0      10.0   6.082763    11.33635   \n",
       "\n",
       "   E_nu_upper   E_nu_avg  d^sigma/dxdQ2   N_events  N_events_errs  N_sys_errs  \\\n",
       "0     40000.0  15037.280       68.08121   1914.486       43.75484         0.0   \n",
       "1     40000.0  12771.790       39.49984  10880.890      104.31150         0.0   \n",
       "2     40000.0   8705.647       28.58960  21806.780      147.67120         0.0   \n",
       "3     40000.0  14379.890       29.13603   6594.826       81.20854         0.0   \n",
       "4     40000.0   8678.035       21.33828  40198.550      200.49580         0.0   \n",
       "\n",
       "   Percent_error_theta  Percent_error_Elepton  Percent_error_Ehadron  \\\n",
       "0                  0.0                    0.0                    0.0   \n",
       "1                  0.0                    0.0                    0.0   \n",
       "2                  0.0                    0.0                    0.0   \n",
       "3                  0.0                    0.0                    0.0   \n",
       "4                  0.0                    0.0                    0.0   \n",
       "\n",
       "   MC_Samples  \n",
       "0     58178.0  \n",
       "1     77752.0  \n",
       "2     83840.0  \n",
       "3     14409.0  \n",
       "4     88220.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = read_txt_pd()\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3fe6f5c-6eab-4517-b58c-e2749df98adc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAP_ERROR_LABEL = {\n",
    "    \"Percent_error_Elepton\": \"El\",\n",
    "    \"Percent_error_theta\": \"theta\",\n",
    "    \"Percent_error_Ehadron\": \"Eh\",\n",
    "    \"Percent_error_combined\": \"comb\",\n",
    "}\n",
    "\n",
    "def load_input(\n",
    "    exp: str = \"FASERv2FCC_wide\",\n",
    "    otype: str = \"inclusive\",\n",
    "    charge: str = \"nu\",\n",
    "    pdfname: str = \"NNPDF40_nnlo_as_01180\",\n",
    "    error: str = \"Percent_error_Elepton\",\n",
    ") -> dict:\n",
    "    # Read and Parse the central values\n",
    "    partial_dataname = f\"{exp}_{otype}_{charge}\"\n",
    "    data_name = f\"diffxsec-{partial_dataname}-a1_{pdfname}\"\n",
    "    path_cv = CURR_PATH.joinpath(f\"pineappl_tables/{data_name}.txt\")\n",
    "    \n",
    "    if charge == \"nu\" or charge == \"nub\":\n",
    "        # Extract the y & central value from pineappl tables\n",
    "        column = 3 if charge == \"nu\" else 4 # Select projectile\n",
    "        x_avg, y_avg, sigma = np.loadtxt(\n",
    "            pathlib.Path(path_cv),\n",
    "            usecols=(0, 1, column),\n",
    "            unpack=True,\n",
    "            skiprows=1,\n",
    "        )\n",
    "    elif charge == \"nochargediscrimination\":\n",
    "        x_avg, y_avg, sigma_nu, sigma_nub = np.loadtxt(\n",
    "            pathlib.Path(path_cv),\n",
    "            usecols=(0, 1, 3, 4),\n",
    "            unpack=True,\n",
    "            skiprows=1,\n",
    "        )\n",
    "        sigma = sigma_nu + sigma_nub\n",
    "    else:\n",
    "        raise ValueEror(f\"{charge} is not valid!\")\n",
    "    \n",
    "    df_predictions = read_txt_pd(exp=exp, otype=otype, charge=charge)\n",
    "    \n",
    "    # Compute the corresponding systematic errors\n",
    "    if error == \"Percent_error_Elepton\":\n",
    "        syst_error = sigma * df_predictions[\"Percent_error_Elepton\"].to_numpy()\n",
    "    elif error == \"Percent_error_theta\":\n",
    "        syst_error = sigma * df_predictions[\"Percent_error_theta\"].to_numpy()\n",
    "    elif error == \"Percent_error_Ehadron\":\n",
    "        syst_error = sigma * df_predictions[\"Percent_error_Ehadron\"].to_numpy()\n",
    "    elif error == \"Percent_error_combined\":\n",
    "        syst_error_El = sigma * df_predictions[\"Percent_error_Elepton\"].to_numpy()\n",
    "        syst_error_Th = sigma * df_predictions[\"Percent_error_theta\"].to_numpy()\n",
    "        syst_error_Eh = sigma * df_predictions[\"Percent_error_Ehadron\"].to_numpy()\n",
    "        syst_error = np.sqrt(syst_error_El**2 + syst_error_Th**2 + syst_error_Eh**2)\n",
    "    else:\n",
    "        raise ValueError(f\"{error} is not a recognized error type!\")\n",
    "    \n",
    "    # Extract the statistical events error\n",
    "    num_events_error = df_predictions[\"N_events_errs\"]\n",
    "    stat_error = 1.0 / num_events_error * sigma\n",
    "    \n",
    "    # Check that the two files have the same knots\n",
    "    np.testing.assert_allclose(x_avg, df_predictions[\"x_avg\"], rtol=5e-3)\n",
    "    \n",
    "    # Add the statistical and systematic in quadrature\n",
    "    comb_error = np.sqrt(syst_error**2 + stat_error**2)\n",
    "    \n",
    "    return {\n",
    "        \"x_values\": df_predictions[\"x_avg\"].to_numpy(),\n",
    "        \"q2_values\": df_predictions[\"Q2_avg\"].to_numpy(),\n",
    "        \"y_values\": y_avg,\n",
    "        \"stat_error\": stat_error.to_numpy(),\n",
    "        \"syst_error\": syst_error,\n",
    "        \"comb_error\": comb_error.to_numpy(),\n",
    "        \"sigma\": sigma,\n",
    "        \"dataset_name\": f\"{partial_dataname}_{MAP_ERROR_LABEL[error]}\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b66bea6c-a4a5-4a39-af74-6dffe7e70690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluctuate_data(central: np.ndarray, comb_error: np.ndarray) -> np.ndarray:\n",
    "    pseudodata = []\n",
    "\n",
    "    for cv, err in zip(central, comb_error):\n",
    "        positive = False\n",
    "\n",
    "        while positive is not True:\n",
    "            shifted_cv = np.random.normal(cv, err)\n",
    "\n",
    "            if shifted_cv > 0:\n",
    "                positive = True\n",
    "\n",
    "        pseudodata.append(shifted_cv)\n",
    "\n",
    "\n",
    "    return np.asarray(pseudodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3898246b-381c-496e-a74f-2db3ad8d7d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fluctuate_data_covmat(central: np.ndarray, covmat: np.ndarray) -> np.ndarray:\n",
    "    cholesky = np.linalg.cholesky(covmat)\n",
    "    positive = False\n",
    "\n",
    "    while positive is not True:\n",
    "        random_samples = np.random.randn(central.shape[0])\n",
    "        shift_data = cholesky @ random_samples\n",
    "        pseudodata = central + shift_data\n",
    "\n",
    "        if np.all(pseudodata >= 0):\n",
    "            positive = True\n",
    "    \n",
    "    return pseudodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c2f4d72-c0f4-4e60-9b3e-15b4f9f26156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to get the Covmat from array\n",
    "get_covmat = lambda arr: np.diag(arr**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33414c37-74bc-4225-b6ee-d512e1582d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_newcommondata(df: pd.DataFrame, folder_name: str):\n",
    "    #  dump the data file\n",
    "    data_central = []\n",
    "    for i in range(len(df[\"G\"])):\n",
    "        data_central.append(float(df.loc[i, \"G\"]))\n",
    "\n",
    "    data_central_yaml = {\"data_central\": data_central}\n",
    "    with open(f\"{folder_name}/data.yaml\", \"w\") as file:\n",
    "        yaml.dump(data_central_yaml, file, sort_keys=False)\n",
    "\n",
    "    # Write kin file\n",
    "    kin = []\n",
    "    for i in range(len(df[\"G\"])):\n",
    "        kin_value = {\n",
    "            \"x\": {\n",
    "                \"min\": None,\n",
    "                \"mid\": float(df.loc[i, \"x\"]),\n",
    "                \"max\": None,\n",
    "            },\n",
    "            \"Q2\": {\"min\": None, \"mid\": float(df.loc[i, \"Q2\"]), \"max\": None},\n",
    "            \"y\": {\"min\": None, \"mid\": float(df.loc[i, \"y\"]), \"max\": None},\n",
    "        }\n",
    "        kin.append(kin_value)\n",
    "\n",
    "    kinematics_yaml = {\"bins\": kin}\n",
    "\n",
    "    with open(f\"{folder_name}/kinematics.yaml\", \"w\") as file:\n",
    "        yaml.dump(kinematics_yaml, file, sort_keys=False)\n",
    "\n",
    "    # Write unc file\n",
    "    error = []\n",
    "    for i in range(len(df)):\n",
    "        e = {\n",
    "            \"stat\": float(df.loc[i, \"stat\"]),\n",
    "            \"sys\": float(df.loc[i, \"sys\"]),\n",
    "        }\n",
    "        error.append(e)\n",
    "\n",
    "    error_definition = {\n",
    "        \"stat\": {\n",
    "            \"description\": \"statistical uncertainty\",\n",
    "            \"treatment\": \"ADD\",\n",
    "            \"type\": \"UNCORR\",\n",
    "        },\n",
    "        \"sys\": {\n",
    "            \"description\": \"systematic uncertainty\",\n",
    "            \"treatment\": \"ADD\",\n",
    "            \"type\": \"UNCORR\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    uncertainties_yaml = {\"definitions\": error_definition, \"bins\": error}\n",
    "\n",
    "    with open(f\"{folder_name}/uncertainties.yaml\", \"w\") as file:\n",
    "        yaml.dump(uncertainties_yaml, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dca61a51-1d53-4e50-865b-371b1400d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_metadata(expname: str, df: pd.DataFrame):      \n",
    "    metadata = {'setname': expname,\n",
    "     'version': 1,\n",
    "     'version_comment': 'Initial implementation',\n",
    "     'iNSPIRE': {'url': ''},\n",
    "     'hepdata': {'url': '', 'version': 1},\n",
    "     'nnpdf_metadata': \n",
    "                {\n",
    "                    'nnpdf31_process': 'DIS CC',\n",
    "                    'experiment': 'FASERV2'\n",
    "                },\n",
    "     'implemented_observables': [{'observable_name': 'fluctuated'.upper(),\n",
    "       'observable': {'description': 'FPF at FCC',\n",
    "        'label': '$d\\sigma / dxdQ^2$',\n",
    "        'units': ''},\n",
    "       'process_type': 'DIS',\n",
    "       'ndata': len(df['G']),\n",
    "       'tables': [0],\n",
    "       'npoints': [0],\n",
    "       'plotting': {'kinematics_override': 'dis_sqrt_scale',\n",
    "        'dataset_label': 'FPF at FCC',\n",
    "        'y_label': '$d\\sigma / dxdQ^2$',\n",
    "        'plot_x': 'Q2',\n",
    "        'line_by': ['x'],\n",
    "        'figure_by': ['k2bins5']},\n",
    "       'kinematic_coverage': ['x', 'Q2', 'y'],\n",
    "       'kinematics': {'variables': {'x': {'description': 'momentum fraction',\n",
    "          'label': '$x$',\n",
    "          'units': ''},\n",
    "         'Q2': {'description': 'virtuality', 'label': '$Q^2$', 'units': '$GeV^2$'},\n",
    "         'y': {'description': 'inelasticity', 'label': '$y$', 'units': ''}},\n",
    "        'file': 'kinematics.yaml'},\n",
    "       'data_central': 'data.yaml',\n",
    "       'data_uncertainties': ['uncertainties.yaml'],\n",
    "       'theory': {\n",
    "           'FK_tables': [[expname]],\n",
    "        'operation': 'null'}}]\n",
    "    }\n",
    "\n",
    "    with open(f\"fluctuated_data/{expname}/metadata.yaml\", \"w\") as file:\n",
    "        yaml.dump(metadata, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aa7a1d76-b2d1-4e72-b303-6485ca44d4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAP_DATASET_NAMES = {\n",
    "    \"FASERv2FCC_wide_inclusive_nu_comb\": 'FASERV2NU_FCC_WIDE_INCLUSIVE',\n",
    "    \"FASERv2FCC_wide_inclusive_nub_comb\": 'FASERV2NB_FCC_WIDE_INCLUSIVE',\n",
    "    \"FASERv2FCC_inclusive_nu_comb\": 'FASERV2NU_FCC_INCLUSIVE',\n",
    "    \"FASERv2FCC_inclusive_nub_comb\": 'FASERV2NB_FCC_INCLUSIVE',\n",
    "    \"FASERv2FCC_deep_inclusive_nu_comb\": 'FASERV2NU_FCC_DEEP_INCLUSIVE',\n",
    "    \"FASERv2FCC_deep_inclusive_nub_comb\": 'FASERV2NB_FCC_DEEP_INCLUSIVE',\n",
    "}\n",
    "\n",
    "def dump_fluctuated_data(\n",
    "    exps: list = [\n",
    "        \"FASERv2FCC_wide\",\n",
    "        \"FASERv2FCC\",\n",
    "        \"FASERv2FCC_deep\",\n",
    "    ],\n",
    "    processes: list = [\"inclusive\"],\n",
    "    charges: list = [\n",
    "        \"nu\",\n",
    "        \"nub\",\n",
    "        # \"nochargediscrimination\"\n",
    "    ],\n",
    "    error: list = [\n",
    "        \"Percent_error_combined\",\n",
    "        # \"Percent_error_Elepton\",\n",
    "        # \"Percent_error_Ehadron\",\n",
    "        # \"Percent_error_theta\",\n",
    "    ],\n",
    "):\n",
    "    for exp, proc, charge, err in product(exps, processes, charges, error):\n",
    "        if exp == \"FASERv\" and proc == \"charm\":\n",
    "            pass\n",
    "        elif exp == \"AdvSND\" and charge == \"nub\":\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Fluctuating '{exp}' - '{proc}' - '{charge}' - '{err}'\")\n",
    "            load_results = load_input(exp=exp, otype=proc, charge=charge, error=err)\n",
    "            \n",
    "            # Compute the covariance matrix using the combined error\n",
    "            # covmat = get_covmat(load_results[\"comb_error\"])\n",
    "    \n",
    "            # Fluctuate the central values\n",
    "            fluctuated_sigma = fluctuate_data(\n",
    "                central=load_results[\"sigma\"],\n",
    "                comb_error=load_results[\"comb_error\"],\n",
    "            )\n",
    "            \n",
    "            # Combine everything into an array\n",
    "            fluctuated_predictions = [\n",
    "                load_results[\"x_values\"],\n",
    "                load_results[\"y_values\"],\n",
    "                load_results[\"q2_values\"],\n",
    "                fluctuated_sigma,\n",
    "                load_results[\"stat_error\"],\n",
    "                load_results[\"syst_error\"],\n",
    "            ]\n",
    "\n",
    "            fluctuated_predictions_df = {\n",
    "                \"x\": load_results[\"x_values\"],\n",
    "                \"y\": load_results[\"y_values\"],\n",
    "                \"Q2\": load_results[\"q2_values\"],\n",
    "                \"G\": fluctuated_sigma,\n",
    "                \"stat\": load_results[\"stat_error\"],\n",
    "                \"sys\": load_results[\"syst_error\"],\n",
    "            }\n",
    "            fluctuated_predictions_df = pd.DataFrame(fluctuated_predictions_df)\n",
    "            \n",
    "            # Dump the final results\n",
    "            # filename = f\"{load_results['dataset_name']}_fluctuated\"\n",
    "            # save_path = CURR_PATH.joinpath(f\"fluctuated_data/{filename}.txt\")\n",
    "            # np.savetxt(save_path, np.column_stack(fluctuated_predictions))\n",
    "            datasetname = MAP_DATASET_NAMES[load_results['dataset_name']]\n",
    "            foldername = f\"fluctuated_data/{datasetname}\"\n",
    "            pathlib.Path(foldername).mkdir(exist_ok=True)\n",
    "            dump_newcommondata(fluctuated_predictions_df, folder_name=foldername)\n",
    "            dump_metadata(datasetname, fluctuated_predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c782c139-f4bb-432a-86d6-640c65da91c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluctuating 'FASERv2FCC_wide' - 'inclusive' - 'nu' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv2FCC_wide' - 'inclusive' - 'nub' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv2FCC' - 'inclusive' - 'nu' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv2FCC' - 'inclusive' - 'nub' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv2FCC_deep' - 'inclusive' - 'nu' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv2FCC_deep' - 'inclusive' - 'nub' - 'Percent_error_combined'\n"
     ]
    }
   ],
   "source": [
    "dump_fluctuated_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3909b9-3ddd-4ed1-a047-95af99142b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
