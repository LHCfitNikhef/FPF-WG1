{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4196a694-2798-4b26-b454-f391a8d7c9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284b53e8-dbad-45ef-973d-c92cb50a5e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(87654321)\n",
    "CURR_PATH = pathlib.Path().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1097bb54-2f73-493e-a2f8-132f858475a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_txt_pd(exp: str = \"FASERv2\", otype: str = \"inclusive\", charge: str = \"nu\") -> pd.DataFrame:\n",
    "    \n",
    "    path = f\"clipped_nan_binned_sysevents_{exp}_{otype}_{charge}.txt\"\n",
    "    fpath = CURR_PATH.joinpath(f\"stat_syst_uncertainties/{path}\")\n",
    "    \n",
    "    colnames = [\n",
    "        \"x_lower\", \n",
    "        \"x_upper\", \n",
    "        \"x_avg\", \n",
    "        \"Q2_lower\", \n",
    "        \"Q2_upper\", \n",
    "        \"Q2_avg\", \n",
    "        \"E_nu_lower\", \n",
    "        \"E_nu_upper\", \n",
    "        \"E_nu_avg\", \n",
    "        \"d^sigma/dxdQ2\", \n",
    "        \"N_events\", \n",
    "        \"N_events_errs\", \n",
    "        \"N_sys_errs\", \n",
    "        \"Percent_error_theta\", \n",
    "        \"Percent_error_Elepton\", \n",
    "        \"Percent_error_Ehadron\", \n",
    "        \"MC_Samples\",\n",
    "    ]\n",
    "    \n",
    "    return pd.read_csv(fpath, skiprows=2, delim_whitespace=True, names=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ad77d5-6ad3-45a4-b687-9326f09ff028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = read_txt_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3fe6f5c-6eab-4517-b58c-e2749df98adc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAP_ERROR_LABEL = {\n",
    "    \"Percent_error_Elepton\": \"El\",\n",
    "    \"Percent_error_theta\": \"El\",\n",
    "    \"Percent_error_Ehadron\": \"Eh\",\n",
    "    \"Percent_error_combined\": \"comb\",\n",
    "}\n",
    "\n",
    "def load_input(\n",
    "    exp: str = \"FASERv2\",\n",
    "    otype: str = \"inclusive\",\n",
    "    charge: str = \"nu\",\n",
    "    pdfname: str = \"NNPDF40_nnlo_as_01180\",\n",
    "    error: str = \"Percent_error_Elepton\",\n",
    ") -> dict:\n",
    "    # Read and Parse the central values\n",
    "    partial_dataname = f\"{exp}_{otype}_{charge}\"\n",
    "    data_name = f\"diffxsec-{partial_dataname}-a1_{pdfname}\"\n",
    "    path_cv = CURR_PATH.joinpath(f\"pineappl_tables/{data_name}.txt\")\n",
    "    \n",
    "    if charge == \"nu\" or charge == \"nub\":\n",
    "        # Extract the y & central value from pineappl tables\n",
    "        column = 3 if charge == \"nu\" else 4 # Select projectile\n",
    "        x_avg, y_avg, sigma = np.loadtxt(\n",
    "            pathlib.Path(path_cv),\n",
    "            usecols=(0, 1, column),\n",
    "            unpack=True,\n",
    "            skiprows=1,\n",
    "        )\n",
    "    elif charge == \"nochargediscrimination\":\n",
    "        x_avg, y_avg, sigma_nu, sigma_nub = np.loadtxt(\n",
    "            pathlib.Path(path_cv),\n",
    "            usecols=(0, 1, 3, 4),\n",
    "            unpack=True,\n",
    "            skiprows=1,\n",
    "        )\n",
    "        sigma = sigma_nu + sigma_nub\n",
    "    else:\n",
    "        raise ValueEror(f\"{charge} is not valid!\")\n",
    "    \n",
    "    df_predictions = read_txt_pd(exp=exp, otype=otype, charge=charge)\n",
    "    \n",
    "    # Compute the corresponding systematic errors\n",
    "    if error == \"Percent_error_Elepton\":\n",
    "        syst_error = sigma * df_predictions[\"Percent_error_Elepton\"].to_numpy()\n",
    "    elif error == \"Percent_error_theta\":\n",
    "        syst_error = sigma * df_predictions[\"Percent_error_theta\"].to_numpy()\n",
    "    elif error == \"Percent_error_Ehadron\":\n",
    "        syst_error = sigma * df_predictions[\"Percent_error_Ehadron\"].to_numpy()\n",
    "    elif error == \"Percent_error_combined\":\n",
    "        syst_error_El = sigma * df_predictions[\"Percent_error_Elepton\"].to_numpy()\n",
    "        syst_error_Th = sigma * df_predictions[\"Percent_error_theta\"].to_numpy()\n",
    "        syst_error_Eh = sigma * df_predictions[\"Percent_error_Ehadron\"].to_numpy()\n",
    "        syst_error = np.sqrt(syst_error_El**2 + syst_error_Th**2 + syst_error_Eh**2)\n",
    "    else:\n",
    "        raise ValueError(f\"{error} is not a recognized error type!\")\n",
    "    \n",
    "    # Extract the statistical events error\n",
    "    num_events_error = df_predictions[\"N_events_errs\"]\n",
    "    stat_error = 1.0 / num_events_error * sigma\n",
    "    \n",
    "    # Check that the two files have the same knots\n",
    "    np.testing.assert_allclose(x_avg, df_predictions[\"x_avg\"], rtol=5e-3)\n",
    "    \n",
    "    # Add the statistical and systematic in quadrature\n",
    "    comb_error = np.sqrt(syst_error**2 + stat_error**2)\n",
    "    \n",
    "    return {\n",
    "        \"x_values\": df_predictions[\"x_avg\"].to_numpy(),\n",
    "        \"q2_values\": df_predictions[\"Q2_avg\"].to_numpy(),\n",
    "        \"y_values\": y_avg,\n",
    "        \"stat_error\": stat_error.to_numpy(),\n",
    "        \"syst_error\": syst_error,\n",
    "        \"comb_error\": comb_error.to_numpy(),\n",
    "        \"sigma\": sigma,\n",
    "        \"dataset_name\": f\"{partial_dataname}_{MAP_ERROR_LABEL[error]}\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3898246b-381c-496e-a74f-2db3ad8d7d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fluctuate_data(central: np.ndarray, covmat: np.ndarray) -> np.ndarray:\n",
    "    cholesky = np.linalg.cholesky(covmat)\n",
    "    random_samples = np.random.randn(central.shape[0])\n",
    "    \n",
    "    shift_data = cholesky @ random_samples\n",
    "    pseudodata = central + shift_data\n",
    "    \n",
    "    return pseudodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6530534f-0615-43e5-b1d2-52b7fb735ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fasernu2_nu = load_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c2f4d72-c0f4-4e60-9b3e-15b4f9f26156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to get the Covmat from array\n",
    "get_covmat = lambda arr: np.diag(arr**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa7a1d76-b2d1-4e72-b303-6485ca44d4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dump_fluctuated_data(\n",
    "    exps: list = [\"FASERv2\", \"FASERv\", \"FLArE100\", \"SND\"],\n",
    "    processes: list = [\"inclusive\", \"charm\"],\n",
    "    charges: list = [\"nu\", \"nub\", \"nochargediscrimination\"],\n",
    "    error: list = [\"Percent_error_combined\"],\n",
    "):\n",
    "    for exp, proc, charge, err in product(exps, processes, charges, error):\n",
    "        print(f\"Fluctuating '{exp}' - '{proc}' - '{charge}' - '{err}'\")\n",
    "        load_results = load_input(exp=exp, otype=proc, charge=charge, error=err)\n",
    "        \n",
    "        # Compute the covariance matrix using the combined error\n",
    "        covmat = get_covmat(load_results[\"comb_error\"])\n",
    "        \n",
    "        # Fluctuate the central values\n",
    "        fluctuated_sigma = fluctuate_data(\n",
    "            central=load_results[\"sigma\"],\n",
    "            covmat=covmat,\n",
    "        )\n",
    "        \n",
    "        # Combine everything into an array\n",
    "        fluctuated_predictions = [\n",
    "            load_results[\"x_values\"],\n",
    "            load_results[\"y_values\"],\n",
    "            load_results[\"q2_values\"],\n",
    "            fluctuated_sigma,\n",
    "            load_results[\"stat_error\"],\n",
    "            load_results[\"syst_error\"],\n",
    "        ]\n",
    "        \n",
    "        # Dump the final results\n",
    "        filename = f\"{load_results['dataset_name']}_fluctuated\"\n",
    "        save_path = CURR_PATH.joinpath(f\"fluctuated_data/{filename}.txt\")\n",
    "        np.savetxt(save_path, np.column_stack(fluctuated_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c782c139-f4bb-432a-86d6-640c65da91c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluctuating 'FASERv2' - 'inclusive' - 'nu' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv2' - 'inclusive' - 'nub' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv2' - 'inclusive' - 'nochargediscrimination' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv2' - 'charm' - 'nu' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv2' - 'charm' - 'nub' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv2' - 'charm' - 'nochargediscrimination' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv' - 'inclusive' - 'nu' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv' - 'inclusive' - 'nub' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv' - 'inclusive' - 'nochargediscrimination' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv' - 'charm' - 'nu' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv' - 'charm' - 'nub' - 'Percent_error_combined'\n",
      "Fluctuating 'FASERv' - 'charm' - 'nochargediscrimination' - 'Percent_error_combined'\n",
      "Fluctuating 'FLArE100' - 'inclusive' - 'nu' - 'Percent_error_combined'\n",
      "Fluctuating 'FLArE100' - 'inclusive' - 'nub' - 'Percent_error_combined'\n",
      "Fluctuating 'FLArE100' - 'inclusive' - 'nochargediscrimination' - 'Percent_error_combined'\n",
      "Fluctuating 'FLArE100' - 'charm' - 'nu' - 'Percent_error_combined'\n",
      "Fluctuating 'FLArE100' - 'charm' - 'nub' - 'Percent_error_combined'\n",
      "Fluctuating 'FLArE100' - 'charm' - 'nochargediscrimination' - 'Percent_error_combined'\n",
      "Fluctuating 'SND' - 'inclusive' - 'nu' - 'Percent_error_combined'\n",
      "Fluctuating 'SND' - 'inclusive' - 'nub' - 'Percent_error_combined'\n",
      "Fluctuating 'SND' - 'inclusive' - 'nochargediscrimination' - 'Percent_error_combined'\n",
      "Fluctuating 'SND' - 'charm' - 'nu' - 'Percent_error_combined'\n",
      "Fluctuating 'SND' - 'charm' - 'nub' - 'Percent_error_combined'\n",
      "Fluctuating 'SND' - 'charm' - 'nochargediscrimination' - 'Percent_error_combined'\n"
     ]
    }
   ],
   "source": [
    "dump_fluctuated_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3909b9-3ddd-4ed1-a047-95af99142b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
