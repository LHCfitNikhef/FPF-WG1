\documentclass[11pt,a4paper]{article}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{float}
\usepackage{afterpage}
\usepackage{amssymb,amsmath}
\usepackage{multirow,booktabs,multirow}
\usepackage{cite}
\usepackage[colorlinks=true, linkcolor=black!50!blue, urlcolor=blue, citecolor=blue, anchorcolor=blue]{hyperref}
\usepackage[font=small,labelfont=bf,margin=0mm,labelsep=period,tableposition=top]{caption}
\usepackage[a4paper,top=3cm,bottom=2.5cm,left=2.5cm,right=2.5cm,bindingoffset=0mm]{geometry}
\setlength{\unitlength}{1mm}

\usepackage{tabularx}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\smallfrac#1#2{\hbox{$\frac{#1}{#2}$}}
\newcommand{\be}{\begin{equation}}
	\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
	\newcommand{\eea}{\end{eqnarray}}
\newcommand{\bi}{\begin{itemize}}
	\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
	\newcommand{\een}{\end{enumerate}}
\newcommand{\la}{\left\langle}
\newcommand{\ra}{\right\rangle}
\newcommand{\lc}{\left[}
\newcommand{\rc}{\right]}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\as}{\alpha_s}
\newcommand{\aq}{\alpha_s\left( Q^2 \right)}
\newcommand{\amz}{\alpha_s\left( M_Z^2 \right)}
\newcommand{\aqq}{\alpha_s \left( Q^2_0 \right)}
\newcommand{\aqz}{\alpha_s \left( Q^2_0 \right)}
\newcommand{\Ord}{\mathcal{O}}
\newcommand{\MSbar}{\overline{\text{MS}}}
\def\toinf#1{\mathrel{\mathop{\sim}\limits_{\scriptscriptstyle
			{#1\rightarrow\infty }}}}
\def\tozero#1{\mathrel{\mathop{\sim}\limits_{\scriptscriptstyle
			{#1\rightarrow0 }}}}
\def\toone#1{\mathrel{\mathop{\sim}\limits_{\scriptscriptstyle
			{#1\rightarrow1 }}}}
\def\frac#1#2{{{#1}\over {#2}}}
\def\gsim{\gtrsim}
\def\lsim{\lesssim}
\newcommand{\mrexp}{\mathrm{exp}}
\newcommand{\dat}{\mathrm{dat}}
\newcommand{\one}{\mathrm{(1)}}
\newcommand{\two}{\mathrm{(2)}}
\newcommand{\art}{\mathrm{art}} 
\newcommand{\rep}{\mathrm{rep}}
\newcommand{\net}{\mathrm{net}}
\newcommand{\stopp}{\mathrm{stop}}
\newcommand{\sys}{\mathrm{sys}}
\newcommand{\stat}{\mathrm{stat}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\pdf}{\mathrm{pdf}}
\newcommand{\tot}{\mathrm{tot}}
\newcommand{\minn}{\mathrm{min}}
\newcommand{\mut}{\mathrm{mut}}
\newcommand{\partt}{\mathrm{part}}
\newcommand{\dof}{\mathrm{dof}}
\newcommand{\NS}{\mathrm{NS}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\gen}{\mathrm{gen}}
\newcommand{\cut}{\mathrm{cut}}
\newcommand{\parr}{\mathrm{par}}
\newcommand{\val}{\mathrm{val}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\checkk}{\mathrm{check}}
\newcommand{\reff}{\mathrm{ref}}
\newcommand{\Mll}{M_{ll}}
\newcommand{\extra}{\mathrm{extra}}
\newcommand{\draft}[1]{}
\newcommand{\comment}[1]{{\bf \it  #1}}
\newcommand{\muf}{\mu_\text{F}}
\newcommand{\mur}{\mu_\text{R}}

\def\beq{\begin{equation}}
	\def\eeq{\end{equation}}


\def\({\left(}
\def\){\right)}
\def\[{\left[}
\def\]{\right]}
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}


%\let\sectionold\section
%\renewcommand\section[2][]{%
	%\sectionold{\boldmath #2}}

\let\oldsubsection\subsection
\renewcommand\subsection[2][\subsectiontoc]{%
	\def\subsectiontoc{#2}%
	\oldsubsection[#1]{\boldmath #2}%
}

\let\oldsubsubsection\subsubsection
\renewcommand\subsubsection[2][\subsubsectiontoc]{%
	\def\subsubsectiontoc{#2}%
	\oldsubsubsection[#1]{\boldmath #2}%
}


\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
\newcommand{\tmtextit}[1]{{\itshape{#1}}}
\newcommand{\tmtextrm}[1]{{\rmfamily{#1}}}
\newcommand{\tmtexttt}[1]{{\ttfamily{#1}}}


\usepackage{xcolor}
\definecolor{tpurple}{RGB}{128,0,128}
\definecolor{darkgreen}{RGB}{0,180,0}
\newcommand{\SM}[1]{\textbf{\textcolor{blue}  {SM: #1}}}
\newcommand{\MB}[1]{\textbf{\textcolor{red}   {MB: #1}}}
\newcommand{\MBn}[2]{\textcolor{red}{OLD: #1 NEW: #2}}
\newcommand{\LR}[1]{{\bf\color{orange}LR: #1}}
\newcommand{\todo}[1]{{\bf\color{red}TODO: #1}}
\newcommand{\JR}[1]{{\bf\color{purple}JR: #1}}
\newcommand{\SF}[1]{{\bf\color{darkgreen}SF: #1}}
\newcommand{\RDB}[1]{{\bf\color{cyan}RDB: #1}}

\begin{document}
	
\noindent
We would like to express our gratitude to the referee for the appreciation
of our work and for providing constructive
comments on our manuscript. The valuable feedback has been used to
improve the quality and clarity of our work.
%
In response to the suggestions, we address below each of the points raised by 
the referee and describe the actions that have been taken
in the revised version of the manuscript.
%
To facilitate the review process, in the revised version of the manuscript
we have highlighted all the changes that have been made with respect to the original
submission.

\noindent

\begin{enumerate}
	\item {\it Page 3, below (2.8). It is noted that nuclear corrections are not included when 
		interpreting the neutrino structure function data in terms of proton PDFs. It would be good
		to provide some brief discussion of the size of these and the associated uncertainties.
	}
	
	In order to address the need for a comment regarding the sizes of the nuclear corrections
	in proton PDF fits, we have swapped the order of the last two sentences in that paragraph.
	The cited references, and the references therein, provide extensive discussions regarding
	the procedure to include nuclear corrections and their effects.
	
	\item {\it Table 2.2. It is not clear to me at the point where these numbers are produced exactly
		how the cross section inputs corresponding to these numbers are calculated. So I
		think a reference forward to Section 2.5, where the theory settings are described, is
		needed. Although even then, for completeness giving the PDF set that is used and
		some uncertain
		ty on the event rates here would be useful.
	}
	
	The numbers in Table 2.2 are computed by integrating the event yields shown in Eq.~2.12 in
	which the differential cross sections are computed using the central value of the PDF4LHC21
	set. For completeness, we have added a sentence above Eq. (2.16) that mention the specific
	PDF set used to compute these numbers, and where we also indicate that the calculation
        is performed at NLO in the QCD expansion.
        %
        Although
	Section 2.5 indeed provides some descriptions regarding the computation of the differential cross sections
	(which were also described in Section 2.1), it mainly focuses on the construction of the pseudodata
	needed for the fit/profiling.

        This said, we would like to emphasize that the predicted event rates in Table 2.2 depend
        only mildly on the specific settings of the DIS cross-section entering the calculation.
        %
        Furthermore, these event rates are only used to determine the expected bib-by-bin
        statistical uncertainty.
        %
        Hence our results are relatively independent on the theoretical settings
        entering the calculation of the event yields in Table 2.2.
	
	\item {\it Page 11. The discussion about consistency between the PDF set and theory settings
		used to produce the pseudodata and those entering the fit/profiling is in my view not
		correct or at least too strong. In particular, while it is perfectly reasonable to keep
		these the same there is definitely no requirement to, as is currently strongly implied in
		the discussion. In real PDF fits we often see that the fit quality for a given dataset does
		not follow textbook expectations, with $\chi^2/N \sim 1$. So some inconsistency between data
		and theory is often observed, rather than being artificial. Indeed, it is precisely because
		of this effect that tolerances (which are included in e.g. the PDF4LHC profiling) are
		included. In other words, one could perfectly reasonably generate pseudodata with a
		different PDF set, or different theory settings in order to emulate this inconsistency.
		One is free not to, but it should not be suggested that complete consistency is the only
		option here. It is a choice that is made, and not the only possible one.
	}
	
          We agree with the referee that, when generating pseudo-data, one may or may not want to
          assume the same underlying truth as in the prior of the corresponding PDF fit.
          %
          Indeed, data inconsistencies are present in real data.
          
          Our motivation to keep these settings consistent is that we would like to investigate
          what is the PDF constraining power of the FPF measurements
          assuming that there are no inconsistencies,
          which is the best-case scenario.
          %
          In the presence of inconsistencies one finds two effects: a shift in the central value
          of the PDF and a reduction of the PDF error whose magnitude depends on the assumed
          inconsistencies.
          %
          For example, if we generate the FPF pseudo-data far from the PDF4LHC21 prior,
          the main impact of the data will be to shift the central prediction, rather than
          to reduce the uncertainty.

          Another reason to assume this consistency  is that when carrying out Hessian profiling,
          one keeps the PDF parametrisation fixed. In the presence of inconsistencies, it may be
          that the best fit with FPF data requires a different parametrisation and hence
          a different PDF error analysis.
          %
          So Hessian profiling is most reliable in the absence of data inconsistencies.

          In the revised version of the paper, we have emphasized our two-fold region
          to assume full consistency between the prior PDF and the FPF pseudo-data: to
          quantify the PDF reach of the FPF pseudo-data in the most advantageous scenario,
          and to ensure the procedural validity of the Hessian profiling procedure.

          This said, the suggestion from the referee to generate data with an ``inconsistent'' PDF
          set is valuable and it would be interesting to carry out this exercise in the future.
          
	\item {\it Page 15, and Fig. 3.3. Perhaps some explanation of why dV and to a lesser extend uV
		benefits from charge-lepton identification could be provided?
	}
	
	We have slightly rephrased the subsequent sentences to make it clear that the improvements
	seen in $d_v$ and to a lesser extend in $u_v$ can be fully understood by looking at analytical 
	LO expressions of the structure functions in terms of the PDFs for $\nu$ and $\bar{\nu}$.
	
	\item {\it Page 16, and Appendix A. The fact that the FASER$\nu$ (and SNDLHC) projections lead
		to a very limited improvement on the PDF uncertainties is rather hidden in a paragraph
		here, and then in the appendix. In my view, this ‘negative’ result should be given more
		prominence. It after all motivates the improvements that might come with the FPF. I
		would suggest moving this to the main body of the text and starting with this as the
		first study.
	}
	
	The main reason to put the study of the constraints provided by FASER$\nu$ in the Appendix is because
	the analysis in Section 3 is mainly dedicated to the FPF experiments. We nevertheless agree with the
	referee that such findings should be highlighted and used as a motivation for the FPF. In this regard, we
	have added two sentences in the introductory paragraph of Section to describe this.
	
	\item {\it Page 22. I am rather unsure about the approach for presenting results here, and in
		particular in showing numbers without including systematic errors, which are described
		as being ‘optimistic’. Having zero systematic uncertainties is surely unrealistic, rather
		than optimistic, so I feel as though a clearer justification for this needs to be given.
		Even more importantly, the labelling of the result without systematic uncertainties as
		‘FPF’ and those with as ‘FPF*’ is surely the wrong way round, given it implies that
		the case where the systematic uncertainties are not included is the default in some
		sense. So these should be swapped, and the rationale behind showing numbers without
		systematic uncertainties accounted for more clearly presented.
	}
	
	The referee is surely correct in that zero-systematic uncertainties is unrealistic. However,
	the reason to also show results with statistical uncertainties only is twofold. First, it
	substantiates the claim that FPF measurements will be statistically dominated. Second, in the
	ideal scenario (most optimistic) in which all the sources of systematic uncertainties are 
	under control, the statistic-only case would be the limit in terms of constraints on PDF
	determination.  It is also worth noting that the estimation of the systematic uncertainties in our 
	analysis is very  much conservative.
	
	Following the referee's suggestion, the results which only includes the statistical errors
	are labelled "FPF$\star$" while the one that also account for the systematics are labelled "FPF".
	
	\item {\it Section 4 and elsewhere. Given these are HL-LHC projections, somewhere these should
		be compared with the HL-LHC PDFs of Ref [34]. This would surely be the fairer
		comparison, or in any case will give a clearer picture of where things may stand.
	}
	
	The reason why we did not provide comparisons with the HL-LHC PDFs of ref. [34] is because
	the pseudodata used to determine these PDFs were produced with the PDF4LHC15 set which
	does not account for the recent LHC measurements. The most consistent comparisons would
	be to re-do the analysis done in ref. [34] using PDF4LHC21 and then compare the resulting PDFs 
	with our determination.

\end{enumerate}

\noindent
We now address the comments of referee B

\begin{enumerate}
\item{\it Page 3, lines 34-36 column 2 “since in general the strange and charm … not expected to vanish”. I agree but I suggest adding a reference or a comment supporting this statement. }

We have added the following references which explore strange and charm PDF asymmetries Sufian:2018cpj, Sufian:2020coz

\item{\it p. 4 l. 30-35 col.2 “Also, to identify… event yields.” Why a cut on the hadronic energy should properly simulate a cut on the number of charged tracks emerging from the interaction vertex?}

The charged track multiplicity is expected to grow with the hadronic system’s invariant mass. We make this clear and cite a neutrino-Hydrogen interaction multiplicity study (Aachen-Bonn-CERN-Munich-Oxford:1981lfk) as well as a FASER paper (FASER:2019dxq) which also point this out. We also append this sentence:

"…, as the charged track multiplicity is expected to grow with $W$~\cite{Aachen-Bonn-CERN-Munich-Oxford:1981lfk,FASER:2019dxq}."


\item{\it Table 2.1 What is the meaning of the asterisk in “FLArE (*)”? I cannot see it referred to in the caption} 

The meaning of the asterisk was to be clear that this row represents 2 proposed detectors, as opposed to the remaining rows which are each one detector. We make this clear by appending the end of the caption:

"…, which we denote for the two detectors as as FLArE(*)."

\item{\it  p.5 l. 38 col.2 “is made of thin sensitive layers” What is the material/detector the layers are made of?}

The target layers are interleaved with tungsten and emulsion. We make this clear by ammending this sentence:

"...sensitive layers of emulsion..."

\item{\it  p.5 l.49 col 1 and col 2 (“AdvSND” and “FLArE”) Can you put a reference about these experiments? Did you take the info from ref [10]?}

We have added the original flare proposal paper as well as the whitepaper (ref[10]) to the FLArE section. For AdvSND, we have added included the whitepaper  

\item{\it p.6 l. 50-53 col.2 “Here we neglect efficiency .. simulation.” I am aware that this might be difficult to simulate at this stage but I think this effect is non-negligible due to misidentification ( a large background could enter this sample). Can you comment further on it? In addition, some guesses on misidentification capabilities can be drawn by the current FASER and SND@CERN capabilities.}

Indeed, misidentification is not expected to be negligible. While there are no detailed studies of charm tagging efficiencies at the FPF, as we point out in the paper charm tagging can be done by through multiple methods, including reconstructing the topology of D-meson decays, and through dimuon events. With multiple techniques, one can draw the conclusion that charm-tagging efficiency is not small.
{\color {red} [MF: looking for quantitative performances/efficiencies, can't find anything. Is there more we can say? We could look at older experiments]}

\item{\it p.9 l. 41-43 col.2 “We note that … in our estimation.” (see also the conclusions p. 27 l. 52-55 col.1) I am puzzled by this statement. It is well known that flux systematics in Faser and SND@CERN play a prominent role and set the normalization (and shape?) of eqn 2.12. How can you ignore such an important effect in your analyses? Even if the PDF constraints are marginally affected by the normalization of the neutrino flux, some of your considerations about the on-axis (faser and faser2) versus the off-axis experiments (SND@CERN and its upgrades) may be affected by flux uncertainties. Can you comment on it in the paper?}

Indeed, neutrino flux uncertainties are well established, and the shape and normalization of the flux can vary widely between different models. It is important to note that neutrino measurements actually constrain the product of flux and cross-section - each of these components brings an uncertainty with it, and a full analysis with real data would constrain them simultaneously. We wish to motivate this joint analysis by calculating the  impact that FPF data can bring to a PDF fit. In this sense, by taking the flux to be known we can understand the full reach of FPF data on cross-section measurements, analogous to what was done in 2309.10417. Moreover, projections of FPF data on flux measurements has shown that HL-LHC data can bring flux uncertainties to a sub-percent level. We thank the referee for this important point and add the following paragraph after we introduce the flux model in the middle of section 2.3. 

"As pointed out in Ref.~\cite{Kling:2021gos} there are notable neutrino flux uncertainties, as various event generators do not agree on the forward parent hadron spectra. The spread of the generators' predictions can be taken as a means of flux uncertainty, in which case there is a $\lesssim 50\%$ uncertainty on the interacting muon neutrino spectrum --- if left unresolved this would be a significant systematic. Indeed, there are already projections of FPF measurements which would reduce this uncertainty to the sub-percent level~\cite{Kling:2023tgr} as well as efforts to describe the uncertainty in a data-driven way while improving the modelling of forward hadronization~\cite{Fieg:2023kld}. However, it is important to note that forward neutrino experiments actually constrain the product of flux and cross-section, and one must be assumed to measure the other. In a full analysis, they would be constrained simultaneously in a joint measurement. In our study, we aim to understand the full impact of FPF data on the PDF fit, thus motivating this future joint measurement. To this aim, we take the neutrino flux to be known and focus on the irreducible systematics associated with event reconstruction." 

\item{\it p.11 l. 33-37 col.1 Are you sure that an assumption of $f_{corr}$ = 0.5 is realistic? The estimates made in ref.[34] are for LHC experiments, not for fixed-target experiments like those of the FPF.}

{\color{red} [MF: can someone comment here?]}

\item{\it  Fig. 2.4 In the bottom-left plot, I see a sharp increase in the fractional error at about x= 4 e-2. Why?}

The points near $x$=4e-2 with large uncertainties have small $E_h$, and are approaching the $E_h>100~ {\rm GeV}$ threshold that we set in Table 2.1. When we fluctuate the data according to our definition of uncertainty (Eq. 2.19 with $E_h$ instead of $E_{\ell}$), these points drop below the acceptance and thus contribute to a fluctuation in this bin.

\item{\it p.19 l.50-53 col 1 and in general along the paper: I cannot see a discussion of the impact of neutral-current events at the FPF. Can you further comment on it?}

There are expected to be roughly as many neutral current neutrino scattering events as charged current events. However, due to the lack of information on both the incoming and outgoing neutrino, the full event cannot be reconstructed, only the total hadronic energy can be measured. One could try to use the total NC event rate as a means of constraining the integrated PDF, however we expect that this would negligibly improve the impact on the PDF constraints as compared to fully reconstructed CC events. {\color{red} [MF: Does this agree with others' understanding, should we add something to the paper? There is the possibility of using muons with NC for PDF constraints, which we could comment on.]}


\end{enumerate}
	
\end{document}
